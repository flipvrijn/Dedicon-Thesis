{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/media/Data/flipvanrijn/datasets/coco/processed/reduced/coco_train_context.pkl', 'r') as f:\n",
    "    titles = cPickle.load(f)\n",
    "    descriptions = cPickle.load(f)\n",
    "    tags = cPickle.load(f)\n",
    "\n",
    "with open('/media/Data/flipvanrijn/datasets/coco/processed/reduced/coco_align.train.pkl', 'rb') as f:\n",
    "    captions = cPickle.load(f)\n",
    "\n",
    "with open('/media/Data/flipvanrijn/datasets/coco/images/train2014list.txt', 'r') as f:\n",
    "    images = f.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im2idx = {}\n",
    "for i, data in enumerate(captions):\n",
    "    cap, im = data\n",
    "    if im in im2idx:\n",
    "        im2idx[im].append(i)\n",
    "    else:\n",
    "        im2idx[im] = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Quadro K2200\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec.load_word2vec_format('/media/Data/flipvanrijn/models/word2vec/enwiki-latest-pages.512.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "with open('/media/Data/flipvanrijn/datasets/coco/processed/reduced/tfidf_model.pkl') as f_in:\n",
    "    tfidf_model = cPickle.load(f_in)\n",
    "tfidf_feature_names = tfidf_model.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'beansprout', 0.29767941588364305), (u'civilis', 0.29767941588364305), (u'food noodl', 0.2927815452498042), (u'ricenoodl', 0.2927815452498042), (u'singapor food', 0.2927815452498042), (u'tasti food', 0.2927815452498042), (u'springrol', 0.27905876959095355), (u'carrot lettuc', 0.2741608989571147), (u'noodl', 0.21099375611502338), (u'lettuc', 0.20440880746711915), (u'food', 0.1962263782009292), (u'tasti', 0.19565741985450608), (u'asian', 0.1891616591684372), (u'singapor', 0.18715731933526472), (u'bowl', 0.15961712112906215), (u'museum', 0.15675210481196644), (u'carrot', 0.14995995793557892), (u'build', 0.14288816270076565)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.tokenize import wordpunct_tokenize, sent_tokenize\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preprocess(s, model):\n",
    "    global stop, stemmer\n",
    "    \n",
    "    ns = []\n",
    "    for w in s:\n",
    "        w = stemmer.stem(w)\n",
    "        if w:\n",
    "            ns.append(w)\n",
    "        \n",
    "    return ns\n",
    "\n",
    "def get_document(title, desc, tags):\n",
    "    context = []\n",
    "    title = preprocess(title.split(' '), model)\n",
    "    if title:\n",
    "        context += title\n",
    "    # ... description (for each sentence) ...\n",
    "    for sent in sent_tokenize(desc):\n",
    "        sent = preprocess(sent.split(' '), model)\n",
    "        if desc:\n",
    "            context += sent\n",
    "    # ... and tagsc\n",
    "    ts = preprocess(tags, model)\n",
    "    if ts:\n",
    "        context += ts\n",
    "    \n",
    "    return ' '.join(context)\n",
    "\n",
    "idx = 40\n",
    "doc = get_document(titles[idx], descriptions[idx], tags[idx])\n",
    "feats = tfidf_model.transform([doc]).todense()[0].tolist()[0]\n",
    "scores = sorted([(tfidf_feature_names[pair[0]], pair[1]) for pair in zip(range(0, len(feats)), feats) if pair[1] > 0], key=lambda x: x[1] * -1)\n",
    "print scores\n",
    "'noodle' in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(s, model):\n",
    "    global stop, stemmer\n",
    "    \n",
    "    ns = []\n",
    "    for w in s:\n",
    "        w = stemmer.lemmatize(w)\n",
    "        if w in model and w not in stop:\n",
    "            ns.append(w)\n",
    "        \n",
    "    return ns\n",
    "\n",
    "def sim(s1, s2, model):\n",
    "    '''\n",
    "    m = np.zeros((len(s1), len(s2)))\n",
    "    for ii in xrange(len(s1)):\n",
    "        for jj in xrange(len(s2)):\n",
    "            if s1[ii] in normalized_vocab and s2[jj] in normalized_vocab:\n",
    "                s1i = normalized_vocab[s1[ii]]\n",
    "                s2j = normalized_vocab[s2[jj]]\n",
    "                m[ii][jj] = model.similarity(s1i, s2j)\n",
    "    return (m.sum() / np.count_nonzero(m))\n",
    "    '''\n",
    "    s1 = preprocess(s1, model)\n",
    "    s2 = preprocess(s2, model)\n",
    "    similarity = model.n_similarity(s1, s2)\n",
    "\n",
    "    return np.sum(similarity)\n",
    "\n",
    "def n_relevant(caption, title, description, tags, model, n=10):\n",
    "    scores = []\n",
    "    w_caption = caption.split(' ')\n",
    "    w_title   = title.split(' ')\n",
    "    if title:\n",
    "        try:\n",
    "            scores.append((sim(w_caption, w_title, model), title))\n",
    "        except:\n",
    "            print w_title\n",
    "            raise\n",
    "    for desc in description.split('.'):\n",
    "        w_desc = desc.split(' ')\n",
    "        if desc:\n",
    "            scores.append((sim(w_caption, w_desc, model), desc))\n",
    "    if tags:\n",
    "        scores.append((sim(w_caption, tags, model), ' '.join(tags)))\n",
    "    \n",
    "    df = pd.DataFrame(scores, columns=['score', 'sentence'])\n",
    "    df = df.dropna()\n",
    "    df = df.sort(['score'], ascending=False)[:n]\n",
    "    return df['score']\n",
    "\n",
    "def n_relevant_words(caption, title, description, tags, model, n=100):\n",
    "    def relevant(s1, s2, model):\n",
    "        m = np.zeros((len(s1), len(s2)))\n",
    "        for ii in xrange(len(s1)):\n",
    "            for jj in xrange(len(s2)):\n",
    "                if s1[ii] in model and s2[jj] in model:\n",
    "                    m[ii][jj] = model.similarity(s1[ii], s2[jj])\n",
    "        scores = []\n",
    "        for r, c in zip(np.argmax(m, axis=0), range(len(s2))):\n",
    "            scores.append((m[r][c], s2[c]))\n",
    "        return scores\n",
    "    \n",
    "    cap = preprocess(caption, model)\n",
    "    title = preprocess(title, model)\n",
    "    descs = description.split('.')\n",
    "    ts = preprocess(tags, model)\n",
    "\n",
    "    scores = []\n",
    "    scores += relevant(cap, title, model)\n",
    "    for desc in descs:\n",
    "        desc = preprocess(desc.split(' '), model)\n",
    "        scores += relevant(cap, desc, model)\n",
    "    scores += relevant(cap, ts, model)\n",
    "\n",
    "    df = pd.DataFrame(scores, columns=['score', 'word'])\n",
    "    df = df.sort(['score'], ascending=False)\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    return df['word'][:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "lengths = []\n",
    "for i in range(len(titles)):\n",
    "    caption_idx = np.random.choice(im2idx[i])\n",
    "    cap = captions[caption_idx][0].split(' ')\n",
    "    title = titles[i].split(' ')\n",
    "    desc = descriptions[i]\n",
    "    ts = tags[i]\n",
    "\n",
    "    relevant_words = n_relevant_words(cap, title, desc, ts, model)\n",
    "    lengths.append(relevant_words.shape[0])\n",
    "    if i % 1000 == 0:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-435-e30f28554c26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mfeat_flatten\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mfeat_flatten\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeat_flatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/site-packages/scipy/sparse/construct.pyc\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m     \"\"\"\n\u001b[1;32m--> 490\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/site-packages/scipy/sparse/construct.pyc\u001b[0m in \u001b[0;36mbmat\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    544\u001b[0m     if (N == 1 and format in (None, 'csr') and all(isinstance(b, csr_matrix)\n\u001b[0;32m    545\u001b[0m                                                    for b in blocks.flat)):\n\u001b[1;32m--> 546\u001b[1;33m         \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_compressed_sparse_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    547\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/site-packages/scipy/sparse/construct.pyc\u001b[0m in \u001b[0;36m_compressed_sparse_stack\u001b[1;34m(blocks, axis)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[0mother_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m     \u001b[0mindptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[0mlast_indptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "images_with_context = []\n",
    "for i, im in enumerate(images):\n",
    "    caption_idx = np.random.choice(im2idx[i])\n",
    "    cap = captions[caption_idx][0].split(' ')\n",
    "    title = titles[i].split(' ')\n",
    "    desc = descriptions[i]\n",
    "    ts = tags[i]\n",
    "\n",
    "    relevant_words = n_relevant_words(cap, title, desc, ts, model)\n",
    "    \n",
    "    if len(relevant_words) > 0:\n",
    "        images_with_context.append(im)\n",
    "        \n",
    "        context = np.zeros((100, 300), dtype=np.float32)\n",
    "        for wi, w in enumerate(relevant_words.values):\n",
    "            context[wi,:] = model[w]\n",
    "\n",
    "        if i == 0:\n",
    "            feat_flatten = csr_matrix(context.flatten())\n",
    "        else:\n",
    "            feat_flatten = vstack([feat_flatten, csr_matrix(context.flatten())])\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<99x30000 sparse matrix of type '<type 'numpy.float32'>'\n",
       " \twith 561600 stored elements in Compressed Sparse Row format>,\n",
       " ['COCO_train2014_000000000009.jpg',\n",
       "  'COCO_train2014_000000000025.jpg',\n",
       "  'COCO_train2014_000000000030.jpg',\n",
       "  'COCO_train2014_000000000034.jpg',\n",
       "  'COCO_train2014_000000000036.jpg',\n",
       "  'COCO_train2014_000000000049.jpg',\n",
       "  'COCO_train2014_000000000061.jpg',\n",
       "  'COCO_train2014_000000000064.jpg',\n",
       "  'COCO_train2014_000000000071.jpg',\n",
       "  'COCO_train2014_000000000072.jpg',\n",
       "  'COCO_train2014_000000000077.jpg',\n",
       "  'COCO_train2014_000000000078.jpg',\n",
       "  'COCO_train2014_000000000081.jpg',\n",
       "  'COCO_train2014_000000000086.jpg',\n",
       "  'COCO_train2014_000000000089.jpg',\n",
       "  'COCO_train2014_000000000092.jpg',\n",
       "  'COCO_train2014_000000000094.jpg',\n",
       "  'COCO_train2014_000000000109.jpg',\n",
       "  'COCO_train2014_000000000110.jpg',\n",
       "  'COCO_train2014_000000000113.jpg',\n",
       "  'COCO_train2014_000000000127.jpg',\n",
       "  'COCO_train2014_000000000138.jpg',\n",
       "  'COCO_train2014_000000000142.jpg',\n",
       "  'COCO_train2014_000000000144.jpg',\n",
       "  'COCO_train2014_000000000149.jpg',\n",
       "  'COCO_train2014_000000000151.jpg',\n",
       "  'COCO_train2014_000000000154.jpg',\n",
       "  'COCO_train2014_000000000165.jpg',\n",
       "  'COCO_train2014_000000000194.jpg',\n",
       "  'COCO_train2014_000000000201.jpg',\n",
       "  'COCO_train2014_000000000247.jpg',\n",
       "  'COCO_train2014_000000000250.jpg',\n",
       "  'COCO_train2014_000000000260.jpg',\n",
       "  'COCO_train2014_000000000263.jpg',\n",
       "  'COCO_train2014_000000000307.jpg',\n",
       "  'COCO_train2014_000000000308.jpg',\n",
       "  'COCO_train2014_000000000309.jpg',\n",
       "  'COCO_train2014_000000000312.jpg',\n",
       "  'COCO_train2014_000000000315.jpg',\n",
       "  'COCO_train2014_000000000321.jpg',\n",
       "  'COCO_train2014_000000000322.jpg',\n",
       "  'COCO_train2014_000000000326.jpg',\n",
       "  'COCO_train2014_000000000332.jpg',\n",
       "  'COCO_train2014_000000000349.jpg',\n",
       "  'COCO_train2014_000000000368.jpg',\n",
       "  'COCO_train2014_000000000370.jpg',\n",
       "  'COCO_train2014_000000000382.jpg',\n",
       "  'COCO_train2014_000000000384.jpg',\n",
       "  'COCO_train2014_000000000389.jpg',\n",
       "  'COCO_train2014_000000000394.jpg',\n",
       "  'COCO_train2014_000000000404.jpg',\n",
       "  'COCO_train2014_000000000419.jpg',\n",
       "  'COCO_train2014_000000000431.jpg',\n",
       "  'COCO_train2014_000000000436.jpg',\n",
       "  'COCO_train2014_000000000438.jpg',\n",
       "  'COCO_train2014_000000000443.jpg',\n",
       "  'COCO_train2014_000000000446.jpg',\n",
       "  'COCO_train2014_000000000450.jpg',\n",
       "  'COCO_train2014_000000000471.jpg',\n",
       "  'COCO_train2014_000000000490.jpg',\n",
       "  'COCO_train2014_000000000491.jpg',\n",
       "  'COCO_train2014_000000000508.jpg',\n",
       "  'COCO_train2014_000000000510.jpg',\n",
       "  'COCO_train2014_000000000514.jpg',\n",
       "  'COCO_train2014_000000000529.jpg',\n",
       "  'COCO_train2014_000000000531.jpg',\n",
       "  'COCO_train2014_000000000532.jpg',\n",
       "  'COCO_train2014_000000000540.jpg',\n",
       "  'COCO_train2014_000000000542.jpg',\n",
       "  'COCO_train2014_000000000560.jpg',\n",
       "  'COCO_train2014_000000000562.jpg',\n",
       "  'COCO_train2014_000000000572.jpg',\n",
       "  'COCO_train2014_000000000575.jpg',\n",
       "  'COCO_train2014_000000000581.jpg',\n",
       "  'COCO_train2014_000000000584.jpg',\n",
       "  'COCO_train2014_000000000595.jpg',\n",
       "  'COCO_train2014_000000000597.jpg',\n",
       "  'COCO_train2014_000000000605.jpg',\n",
       "  'COCO_train2014_000000000612.jpg',\n",
       "  'COCO_train2014_000000000620.jpg',\n",
       "  'COCO_train2014_000000000625.jpg',\n",
       "  'COCO_train2014_000000000629.jpg',\n",
       "  'COCO_train2014_000000000634.jpg',\n",
       "  'COCO_train2014_000000000643.jpg',\n",
       "  'COCO_train2014_000000000650.jpg',\n",
       "  'COCO_train2014_000000000656.jpg',\n",
       "  'COCO_train2014_000000000659.jpg',\n",
       "  'COCO_train2014_000000000670.jpg',\n",
       "  'COCO_train2014_000000000671.jpg',\n",
       "  'COCO_train2014_000000000673.jpg',\n",
       "  'COCO_train2014_000000000681.jpg',\n",
       "  'COCO_train2014_000000000684.jpg',\n",
       "  'COCO_train2014_000000000690.jpg',\n",
       "  'COCO_train2014_000000000706.jpg',\n",
       "  'COCO_train2014_000000000714.jpg',\n",
       "  'COCO_train2014_000000000716.jpg',\n",
       "  'COCO_train2014_000000000723.jpg',\n",
       "  'COCO_train2014_000000000731.jpg',\n",
       "  'COCO_train2014_000000000735.jpg'])"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-339-ac59cb05ed21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcaption_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim2idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcaption\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcaption_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_relevant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-319-2a0d8c649ed3>\u001b[0m in \u001b[0;36mn_relevant\u001b[1;34m(caption, title, description, tags, model, n)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mdropna\u001b[1;34m(self, axis, how, thresh, subset, inplace)\u001b[0m\n\u001b[0;32m   2784\u001b[0m                 \u001b[0magg_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2786\u001b[1;33m             \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magg_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2788\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mthresh\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self, axis, level, numeric_only)\u001b[0m\n\u001b[0;32m   4248\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4249\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4250\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnotnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4251\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4252\u001b[0m                 \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnotnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mstat_func\u001b[1;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m   4253\u001b[0m                                               skipna=skipna)\n\u001b[0;32m   4254\u001b[0m                 return self._reduce(f, name, axis=axis,\n\u001b[1;32m-> 4255\u001b[1;33m                                     skipna=skipna, numeric_only=numeric_only)\n\u001b[0m\u001b[0;32m   4256\u001b[0m             \u001b[0mstat_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4257\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstat_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m   4366\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_coerce_to_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4368\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4370\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0midxmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    218\u001b[0m                                        raise_cast_failure=True)\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, block, axis, do_integrity_check, fastpath)\u001b[0m\n\u001b[0;32m   3378\u001b[0m                 \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3380\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBlock\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3381\u001b[0m             block = make_block(block,\n\u001b[0;32m   3382\u001b[0m                                \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for i in xrange(len(titles)):\n",
    "    caption_idx = np.random.randint(len(im2idx[i]))\n",
    "    caption = captions[caption_idx][0]\n",
    "    scores.append(n_relevant(caption, titles[i], descriptions[i], tags[i], model))\n",
    "    if i % 1000 == 0:\n",
    "        print i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table\n",
      "prep\n",
      "0.0833333333333 0.0909090909091 0.142857142857\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lst = LancasterStemmer()\n",
    "sst = SnowballStemmer(\"english\")\n",
    "print wnl.lemmatize('tables')\n",
    "print lst.stem('preparing')\n",
    "model.similarity('kitchen', 'cake')\n",
    "\n",
    "a = wordnet.synsets('dessert')[0]\n",
    "b = wordnet.synsets('cakes')[0]\n",
    "c = wordnet.synset('prepare.v.03')\n",
    "d = wordnet.synset('frosting.n.01')\n",
    "print b.path_similarity(a), c.path_similarity(a), d.path_similarity(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summed_scores = []\n",
    "for s in scores:\n",
    "    if len(s) > 0:\n",
    "        summed_scores.append(sum(s) / len(s))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(summed_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n"
     ]
    }
   ],
   "source": [
    "scores_mismatch = []\n",
    "for i in xrange(5000):\n",
    "    choice_idxs = range(len(titles))\n",
    "    del choice_idxs[i] # remove current\n",
    "    caption_idx = np.random.randint(len(im2idx[np.random.choice(choice_idxs)]))\n",
    "    caption = captions[caption_idx][0]\n",
    "    scores_mismatch.append(n_relevant(caption, titles[i], descriptions[i], tags[i], model))\n",
    "    if i % 1000 == 0:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summed_scores_mismatch = []\n",
    "for s in scores_mismatch:\n",
    "    if len(s) > 0:\n",
    "        summed_scores_mismatch.append(sum(s) / len(s))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(summed_scores_mismatch)\n",
    "plt.hist(summed_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(summed_scores)), summed_scores)\n",
    "plt.scatter(range(len(summed_scores_mismatch)), summed_scores_mismatch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# context size\n",
    "sizes = []\n",
    "for i in xrange(len(titles)):\n",
    "    count = 0\n",
    "    if titles[i]:\n",
    "        count += len(titles[i].split(' '))\n",
    "    if descriptions[i]:\n",
    "        for s in descriptions[i].split('.'):\n",
    "            for w in s.split(\" \"):\n",
    "                count += 1\n",
    "    if tags[i]:\n",
    "        count += len(tags[i])\n",
    "    sizes.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(sizes, bins=100, range=(0, 100), cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2359908"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Metric evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "sys.path.append('../server/pycocoevalcap/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load captions\n",
    "import cPickle\n",
    "with open('/media/Data/flipvanrijn/datasets/coco/processed/reduced/coco_align.train.pkl') as f:\n",
    "    cap = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'a restaurant has modern wooden tables and chairs .',\n",
       " u'a long restaurant table with rattan rounded back chairs .',\n",
       " u'a long table with a plant on top of it surrounded with wooden chairs',\n",
       " u'a long table with a flower arrangement in the middle for meetings',\n",
       " u'a table is adorned with wooden chairs with blue accents .']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load context\n",
    "with open('/media/Data/flipvanrijn/datasets/coco/processed/reduced/coco_train_context.pkl') as f:\n",
    "    titles = cPickle.load(f)\n",
    "    descs = cPickle.load(f)\n",
    "    tags = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load train list\n",
    "featdict = OrderedDict()\n",
    "idx = 0\n",
    "with open('/media/Data/flipvanrijn/datasets/coco/processed/reduced/train2014list.txt', 'r') as f: #    ls train > train2014list.txt\n",
    "    for line in f:\n",
    "        line = re.sub(r'\\/.*\\/','',line).strip()\n",
    "        featdict[line] = idx\n",
    "        idx += 1\n",
    "# Load split\n",
    "split = []\n",
    "with open('/media/Data/flipvanrijn/datasets/coco/processed/reduced/splits/coco_train.txt', 'r') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        split.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "it = itertools.groupby(cap, operator.itemgetter(1))\n",
    "grouped_caps = {}\n",
    "for k, subit in it:\n",
    "    grouped_caps[k] = [i[0] for i in subit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long dining table annie smithers bistrot we started with the cheese puffs , and it was like biting into little puffs of air . it was so light that spearing it with a fork was too hard , and we had to resort to fingers . we suspect that it is a basic souffle mix dollopped into hot oil and fried to a light and airy puff . amazing . once i saw that the cheese puffs were a manageable size , i immediately added the duck neck sausage to our order . not long after , 4 small discs of minced pork appeared . i love a good sausage and this did disappoint . it was served with segments of orange that provided a nice foil . there were also a few boiled baby beets hidden under the leaves and it seemed a bit too tender compared to the nice big flavoursome beets we were used to . julia was very impressed with the duck fillets , and she remarked several times how tasty the flesh was . i prefered the smoky grilled quail , but it was true , the quail meat was tender and juicy like the duck , but did have the richness of flavour of the duck . the pommes anna under the quail went amazingly well with the madeira sauce . the crispy bits of potato on the edges were also very good . the french beans were tender and so full of spring sweetness . we ended our meal with a good coffee and tea , and the profiteroles to share . we both loved the chocolate sauce , but we prefer the choux pastry at laurent pattiserie . annie smither bistrot produce 72 piper st kyneton vic 3444 reviews annie smithers bistrot , by necia wilden , the age epicure , september 27 , 2005 score annie smithers bistrot the age good food guide 2009 1 chefs hat annie smithers bistrot , kyneton the breakfast blog , saturday , may 13 , 2006 chicken livers , bacon and spinach on toast . one of several tempting dishes on offer at annie smithers bistrot . i love the smell of offal in the morning . mmm liver annie smithers bistrot mietta good gutsy french based dishes annie bistrot gourmet traveller annie smithers , another stephanie alexander alum , is consolidating her empire , a shop and bistro showcasing central victorian produce . assured cooking means primary flavours shine succulent , flaky trout almondine tastes sweet scallops cooked are plated with discs of smoky chorizo tomato tatin is the pick of the entr√©es . usually offal on offer , perhaps creamy brains wrapped in prosciutto , and veal schnitzel , topped with a fried egg and anchovies , is and sized but . strawberry vacherin elevates berries and cream to a fitting conclusion to the meal simple , comforting , classy . food photos cheese puffs with tomato and chilli dipping sauce insides duck neck sausage stuffed with pork mince and pistachios with babybeets , green leaves and orange quails par boned and grilled , served on pommes anna , with grilled mushrooms and madeira sauce aud30 duck fillet with orange marmalade glaze , potatoes profiteroles filled with vanilla ice cream and drizzled with warm dark chocolate sauce long black aud3 mariage freres tea decor photos specials board long dining table object business card back produce store a message from stephanie gumboots stephanie alexander kitchen garden foundation fundraiser aud40 [u'anniesmithersbistro', u'kyneton', u'french', u'restaurant', u'bistro', u'theagegoodfoodguide2009', u'onechefhat', u'decor', u'interior']\n",
      "cakes fresh from the kitchen noisette the pastry chef transferring them to the cakes display . i have always had a fancy for a nice bit of french pastry , and had been wanting to try it out after reading the review in epicure for a while now . what better time than a crisp saturday morning to make the trek to port melbourne for a taste . we were greeted by a cosy little cafe where some staff even spoke french . a good sign of things to come . the coffee , di manfredi , was smooth and rich . the almond danish had a luscious custard sitting on a buttery puff pastry that had a bit of bite to it , rather that the crisp kind that just falls to bits . this was followed by our sausage roll that was topped with linseed which gave it a nice nutty flavour . as some layers of crisp pastry fell away from the top , you could definitely taste the aroma of a good pastry ! noisette bakery boulangerie patisserie 03 9646 9555 84 bay st port melbourne vic 3207 reviews noisette boulangerie patisserie , by matt preston , epicure , the age , april 7 , 2009 pleasure and pain , by john lethlean , epicure , the age , may 12 , 2007 my started to bake at home in 1825 for his neighbour . word got around and such was the demand for his bread that he started the first bakery of the menards , which began the curse of the bread . [u'noisettebakeryboulangeriepatisserie', u'noisette', u'bakery', u'boulangerie', u'patisserie', u'portmelbourne', u'melbourne', u'cafe', u'food', u'cake', u'sweet', u'pastry', u'dessert']\n",
      " phone cell again . [u'england', u'personen', u'person', u'london', u'persons', u'people', u'telefon', u'telephonecell', u'telephone', u'telefonzelle']\n",
      "  [u'kitchen', u'bampton', u'oxfordshire', u'oldfashioned']\n",
      "cooking team  [u'thanksgiving', u'arva', u'farmmommy', u'cooking', u'kitchen', u'grandmother', u'granddaughter', u'eden']\n",
      "business man wearing glasses ! [u'cokebottleglasses', u'business', u'man', u'tie', u'briefcase', u'vintage', u'photo', u'people']\n",
      "clock dining table beaconsfield pde we were taking a walk on beaconsfield parade and we happened on an auction . curious as to the value of a beachfront property , we wandered in for a and stuck around for the auction . the layout was quite amazing , giving this tiny victorian terrace a spacious feel . the near immaculate renovations left probably only the floorboards and the facade . it was passed in at the vendor opening bid of million . ? ? ultimate beachfront living ! auction 4 october 2008 106 beaconsfield parade albert park featuring 3 bedrooms , 2 bathrooms , walk down cellar , inground jet pool vehicle access . capturing panoramic bay views from the upstairs room this superbly designed victorian offers the best of today designer appointments . splendid light filled accomodation comprises downstairs , 3 excellent bedrooms spacious master suite with exquisite ensuite wir , 2nd modern family bathroom separate laundry . upstairs includes an entertainers kitchen opening to expansive room overlooking port phillip bay , powderoom , large separate sitting room leading to sunny terrace . note hydronic heating air conditioning polished floorboards . hocking stuart albert park sales michael coen 0418353110 , 03 9690 5366 update it sold at million ! [u'clock', u'dining', u'table', u'tulips', u'wood', u'floorboard', u'flower', u'antique']\n",
      "esperando  [u'autobus', u'bus', u'calle', u'dublin', u'jwalker', u'person', u'persona', u'road', u'semaforo', u'trafficlight']\n",
      "clean bike , dirty kitchen i did a bit of maintence on the bike . without a garage to work in , one must make do . [u'canonef1740mmf4lusm', u'project365', u'toronto', u'kitchen', u'bike', u'bicycle']\n",
      "taking orders gumbo kitchen i joined the queues at knox lane , melbourne central to get a taste of the of the gumbo kitchen food truck . delicious , yes , worth the 10 minute queue , and 15 minute cooking time , maybe gumbo kitchen 0411037652 164 high st , northcote , vic 3070 gumbo kitchen on facebook gumbokitchen reviews gumbo kitchen urbanspoon [u'gumbokitchenfoodtruck', u'gumbokitchen', u'knoxlane', u'melbournevic', u'melbournecentralshoppingcentre', u'melbournecentral', u'melbourne', u'vic']\n",
      "----------\n",
      "[u'a big red telephone booth that a man is standing in', u'a person standing inside of a phone booth', u'this is an image of a man in a phone booth .', u'a man is standing in a red phone booth .', u'a man using a phone in a phone booth .']\n"
     ]
    }
   ],
   "source": [
    "ordered_context = [None]*len(split)\n",
    "for idx, fname in enumerate(split[:10]):\n",
    "    print titles[featdict[fname]], descs[featdict[fname]], tags[featdict[fname]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "loader = np.load('/media/Data/flipvanrijn/datasets/coco/processed/reduced/context_train_filtered_stemmed.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = loader['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.065151052792187983, array([ 0.06515105]))\n",
      "a couple of people sitting at a table with food in front of them\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import wordpunct_tokenize, sent_tokenize\n",
    "from PIL import Image\n",
    "\n",
    "from rouge.rouge import Rouge\n",
    "e = Rouge()\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem(cap):\n",
    "    cap = cap.split(' ')\n",
    "    stemmed_cap = [stemmer.stem(w) for w in cap]\n",
    "    return ' '.join(stemmed_cap)\n",
    "\n",
    "idx = 1\n",
    "\n",
    "image_path = '/media/Data/flipvanrijn/datasets/coco/images/train/'+split[idx]\n",
    "img = Image.open(image_path)\n",
    "\n",
    "c = '''a close up of a person eating food\n",
    "a couple of people that are in a kitchen\n",
    "a close up of a person cutting a cake\n",
    "a couple of people that are in a kitchen .\n",
    "a person sitting at a table with a plate of food\n",
    "a close up of a person holding a plate of food\n",
    "a couple of people sitting at a table with food .\n",
    "a man sitting at a table with a plate of food\n",
    "a man sitting at a table with a plate of food .\n",
    "a person sitting at a table with a plate of food .\n",
    "a man sitting at a table with a bowl of food .\n",
    "a man sitting at a table with a cake and candles .\n",
    "a man sitting at a table with a tray of food .\n",
    "a couple of people sitting at a table with food on it .\n",
    "a man sitting at a table in front of a plate of food .\n",
    "a person sitting at a table in front of a plate of food .\n",
    "a couple of people sitting at a table with food in front of them\n",
    "a person sitting at a table with a plate of food and drinks .\n",
    "a couple of people sitting at a table with food in front of them .\n",
    "a person sitting at a table with a plate of food in front of him\n",
    "a man sitting at a table with a plate of food in front of him .\n",
    "a person sitting at a table with a plate of food in front of him .\n",
    "a man sitting at a table with a plate of food in front of it .\n",
    "a person sitting at a table with a plate of food in front of it .\n",
    "a man sitting at a table with a plate of food in front of her .'''.split('\\n')\n",
    "cc = ' '.join(data[featdict[split[idx]]]) # cap context\n",
    "\n",
    "refs = {0: c[15:16]}\n",
    "hyp = {0: [cc]}\n",
    "\n",
    "print e.compute_score(refs, hyp)\n",
    "print c[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "server_data = {\n",
    "    'pixels': img.tobytes(), \n",
    "    'size': img.size, \n",
    "    'mode': img.mode, \n",
    "    'introspect': False, \n",
    "    'file_path': '/home/flipvanrijn/',\n",
    "    'k': 25,\n",
    "    #'text_context': np.array(data_ctx[index].todense().reshape((100, 512))),\n",
    "}\n",
    "server_data = cPickle.dumps(server_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man sitting at a table with a plate of food in front of her .\n"
     ]
    }
   ],
   "source": [
    "import socket, struct\n",
    "HOST, PORT = \"localhost\", 9999\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "try:\n",
    "    # Connect to server and send data\n",
    "    sock.connect((HOST, PORT))\n",
    "    server_data = struct.pack('>I', len(server_data)) + server_data\n",
    "    sock.sendall(server_data)\n",
    "\n",
    "    # Receive data from the server and shut down\n",
    "    received = sock.recv(1024)\n",
    "    print received\n",
    "finally:\n",
    "    sock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('table', 'JJ')]"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.pos_tag(['table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = np.load('../dataset/coco/out.npz')\n",
    "nouns = l['nouns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we started with the cheese puffs , and it was like biting into little puffs of air . it was so light that spearing it with a fork was too hard , and we had to resort to fingers . we suspect that it is a basic souffle mix dollopped into hot oil and fried to a light and airy puff . amazing . once i saw that the cheese puffs were a manageable size , i immediately added the duck neck sausage to our order . not long after , 4 small discs of minced pork appeared . i love a good sausage and this did disappoint . it was served with segments of orange that provided a nice foil . there were also a few boiled baby beets hidden under the leaves and it seemed a bit too tender compared to the nice big flavoursome beets we were used to . julia was very impressed with the duck fillets , and she remarked several times how tasty the flesh was . i prefered the smoky grilled quail , but it was true , the quail meat was tender and juicy like the duck , but did have the richness of flavour of the duck . the pommes anna under the quail went amazingly well with the madeira sauce . the crispy bits of potato on the edges were also very good . the french beans were tender and so full of spring sweetness . we ended our meal with a good coffee and tea , and the profiteroles to share . we both loved the chocolate sauce , but we prefer the choux pastry at laurent pattiserie . annie smither bistrot produce 72 piper st kyneton vic 3444 reviews annie smithers bistrot , by necia wilden , the age epicure , september 27 , 2005 score annie smithers bistrot the age good food guide 2009 1 chefs hat annie smithers bistrot , kyneton the breakfast blog , saturday , may 13 , 2006 chicken livers , bacon and spinach on toast . one of several tempting dishes on offer at annie smithers bistrot . i love the smell of offal in the morning . mmm liver annie smithers bistrot mietta good gutsy french based dishes annie bistrot gourmet traveller annie smithers , another stephanie alexander alum , is consolidating her empire , a shop and bistro showcasing central victorian produce . assured cooking means primary flavours shine succulent , flaky trout almondine tastes sweet scallops cooked are plated with discs of smoky chorizo tomato tatin is the pick of the entr√©es . usually offal on offer , perhaps creamy brains wrapped in prosciutto , and veal schnitzel , topped with a fried egg and anchovies , is and sized but . strawberry vacherin elevates berries and cream to a fitting conclusion to the meal simple , comforting , classy . food photos cheese puffs with tomato and chilli dipping sauce insides duck neck sausage stuffed with pork mince and pistachios with babybeets , green leaves and orange quails par boned and grilled , served on pommes anna , with grilled mushrooms and madeira sauce aud30 duck fillet with orange marmalade glaze , potatoes profiteroles filled with vanilla ice cream and drizzled with warm dark chocolate sauce long black aud3 mariage freres tea decor photos specials board long dining table object business card back produce store a message from stephanie gumboots stephanie alexander kitchen garden foundation fundraiser aud40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'annie',\n",
       " u'smithers',\n",
       " u'puffs',\n",
       " u'biting',\n",
       " u'puffs',\n",
       " u'air',\n",
       " u'fork',\n",
       " u'fingers',\n",
       " u'souffle',\n",
       " u'mix',\n",
       " u'oil',\n",
       " u'light',\n",
       " u'airy',\n",
       " u'puff',\n",
       " u'puffs',\n",
       " u'size',\n",
       " u'duck',\n",
       " u'neck',\n",
       " u'sausage',\n",
       " u'order',\n",
       " u'discs',\n",
       " u'pork',\n",
       " u'sausage',\n",
       " u'disappoint',\n",
       " u'segments',\n",
       " u'foil',\n",
       " u'baby',\n",
       " u'beets',\n",
       " u'leaves',\n",
       " u'bit',\n",
       " u'flavoursome',\n",
       " u'beets',\n",
       " u'julia',\n",
       " u'duck',\n",
       " u'fillets',\n",
       " u'times',\n",
       " u'tasty',\n",
       " u'flesh',\n",
       " u'smoky',\n",
       " u'quail',\n",
       " u'quail',\n",
       " u'meat',\n",
       " u'juicy',\n",
       " u'duck',\n",
       " u'richness',\n",
       " u'duck',\n",
       " u'pommes',\n",
       " u'quail',\n",
       " u'madeira',\n",
       " u'sauce',\n",
       " u'crispy',\n",
       " u'potato',\n",
       " u'edges',\n",
       " u'beans',\n",
       " u'tender',\n",
       " u'spring',\n",
       " u'sweetness',\n",
       " u'meal',\n",
       " u'coffee',\n",
       " u'tea',\n",
       " u'profiteroles',\n",
       " u'chocolate',\n",
       " u'sauce',\n",
       " u'choux',\n",
       " u'pastry',\n",
       " u'laurent',\n",
       " u'pattiserie',\n",
       " u'annie',\n",
       " u'smither',\n",
       " u'bistrot',\n",
       " u'produce',\n",
       " u'piper',\n",
       " u'st',\n",
       " u'kyneton',\n",
       " u'vic',\n",
       " u'reviews',\n",
       " u'smithers',\n",
       " u'necia',\n",
       " u'age',\n",
       " u'epicure',\n",
       " u'september',\n",
       " u'score',\n",
       " u'annie',\n",
       " u'smithers',\n",
       " u'age',\n",
       " u'good',\n",
       " u'food',\n",
       " u'guide',\n",
       " u'chefs',\n",
       " u'annie',\n",
       " u'smithers',\n",
       " u'kyneton',\n",
       " u'breakfast',\n",
       " u'blog',\n",
       " u'saturday',\n",
       " u'chicken',\n",
       " u'livers',\n",
       " u'bacon',\n",
       " u'spinach',\n",
       " u'toast',\n",
       " u'dishes',\n",
       " u'offer',\n",
       " u'annie',\n",
       " u'smithers',\n",
       " u'smell',\n",
       " u'morning',\n",
       " u'mmm',\n",
       " u'liver',\n",
       " u'annie',\n",
       " u'smithers',\n",
       " u'mietta',\n",
       " u'good',\n",
       " u'gutsy',\n",
       " u'french',\n",
       " u'dishes',\n",
       " u'bistrot',\n",
       " u'gourmet',\n",
       " u'traveller',\n",
       " u'annie',\n",
       " u'smithers',\n",
       " u'stephanie',\n",
       " u'alexander',\n",
       " u'alum',\n",
       " u'empire',\n",
       " u'shop',\n",
       " u'bistro',\n",
       " u'victorian',\n",
       " u'produce',\n",
       " u'means',\n",
       " u'flavours',\n",
       " u'succulent',\n",
       " u'almondine',\n",
       " u'tastes',\n",
       " u'scallops',\n",
       " u'discs',\n",
       " u'chorizo',\n",
       " u'tomato',\n",
       " u'tatin',\n",
       " u'pick',\n",
       " u'entr\\xe9es',\n",
       " u'offer',\n",
       " u'brains',\n",
       " u'prosciutto',\n",
       " u'veal',\n",
       " u'schnitzel',\n",
       " u'egg',\n",
       " u'anchovies',\n",
       " u'strawberry',\n",
       " u'vacherin',\n",
       " u'berries',\n",
       " u'cream',\n",
       " u'fitting',\n",
       " u'conclusion',\n",
       " u'meal',\n",
       " u'simple',\n",
       " u'food',\n",
       " u'photos',\n",
       " u'puffs',\n",
       " u'tomato',\n",
       " u'chilli',\n",
       " u'sauce',\n",
       " u'insides',\n",
       " u'neck',\n",
       " u'sausage',\n",
       " u'pork',\n",
       " u'mince',\n",
       " u'pistachios',\n",
       " u'babybeets',\n",
       " u'leaves',\n",
       " u'orange',\n",
       " u'quails',\n",
       " u'par',\n",
       " u'pommes',\n",
       " u'mushrooms',\n",
       " u'madeira',\n",
       " u'sauce',\n",
       " u'duck',\n",
       " u'fillet',\n",
       " u'marmalade',\n",
       " u'glaze',\n",
       " u'profiteroles',\n",
       " u'vanilla',\n",
       " u'ice',\n",
       " u'cream',\n",
       " u'warm',\n",
       " u'dark',\n",
       " u'chocolate',\n",
       " u'sauce',\n",
       " u'mariage',\n",
       " u'freres',\n",
       " u'tea',\n",
       " u'decor',\n",
       " u'photos',\n",
       " u'specials',\n",
       " u'object',\n",
       " u'business',\n",
       " u'card',\n",
       " u'message',\n",
       " u'stephanie',\n",
       " u'gumboots',\n",
       " u'alexander',\n",
       " u'foundation',\n",
       " u'fundraiser',\n",
       " u'anniesmithersbistro',\n",
       " u'french',\n",
       " u'restaurant',\n",
       " u'bistro',\n",
       " u'decor']"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print descs[featdict[split[0]]]\n",
    "nouns[featdict[split[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
