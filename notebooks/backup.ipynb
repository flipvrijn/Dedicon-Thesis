{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Encoder keras snippets###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old code:\n",
    "\n",
    "    image_model = Sequential()\n",
    "    image_model.add(Dense(4096, EMBEDDING_SIZE, init='uniform', activity_regularizer=l2(1e-3)))\n",
    "    f_image = theano.function([image_model.get_input(train=False)], image_model.get_output(train=False))\n",
    "    img_output = f_image(image_feats) # CNN input w/ shape (nb_regions, nb_features)\n",
    "    img_output.shape\n",
    "    \n",
    "    print image_model.layers[0].W.get_value().shape, image_model.layers[0].b.get_value().shape\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.imshow(image_model.layers[0].W.get_value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old code:\n",
    "    \n",
    "    from networks.keras.birnn import BiDirectionLSTM\n",
    "    t_b = time.time()\n",
    "    sent_model = Sequential()\n",
    "    sent_model.add(BiDirectionLSTM(8856, EMBEDDING_SIZE, activation='relu', output_mode='sum', inner_activation='relu'))\n",
    "    f_sent = theano.function([sent_model.get_input(train=False)], sent_model.get_output(train=False))\n",
    "    print 'Compiled BiRNN in {}s'.format(time.time() - t_b)\n",
    "    \n",
    "    t_b = time.time()\n",
    "    sent_output = f_sent(sentence_1h)\n",
    "    print sent_output.shape\n",
    "    print 'Sentence embedding in {}: {}'.format(time.time() - t_b, sent_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old code:\n",
    "\n",
    "    # Compute the word/image scores\n",
    "    ''' \n",
    "    Foreach word s_t:\n",
    "        max v_i*s_t\n",
    "    '''\n",
    "    nb_img_fragments = img_output.shape[0]\n",
    "    nb_sent_fragments = sent_output.shape[0]\n",
    "    scores = np.empty((nb_img_fragments, nb_sent_fragments), dtype=np.float32)\n",
    "    for t in xrange(nb_sent_fragments):\n",
    "        for i in xrange(nb_img_fragments):\n",
    "            fragment_out = img_output[i]\n",
    "            sent = sent_output[t]\n",
    "            scores[i][t] = np.dot(fragment_out, np.squeeze(sent))\n",
    "    print scores.shape\n",
    "    max_scores = np.amax(scores, axis=0)[np.newaxis, :]\n",
    "    plt.gray()\n",
    "    fig_feats, ax_feats = plt.subplots(1, 2)\n",
    "    ax_feats[0].imshow(scores)\n",
    "    ax_feats[1].imshow(max_scores)\n",
    "    ax_feats[1].axis('off')\n",
    "    idx_fragment = np.argmax(scores, axis=0)\n",
    "    print 'S_kl = {}'.format(np.sum(max_scores))\n",
    "\n",
    "    # Plot the images with their words\n",
    "    fig_fragments, ax_fragments = plt.subplots(1, len(idx_fragment))\n",
    "    fig_fragments.set_size_inches(15, 2)\n",
    "    for i, idx in enumerate(idx_fragment):\n",
    "        ax_fragments[i].imshow(image_fragments[idx])\n",
    "        ax_fragments[i].yaxis.set_visible(False)\n",
    "        ax_fragments[i].set_title(sentence[i])\n",
    "        ax_fragments[i].set_xticklabels([])\n",
    "        ax_fragments[i].set_xlabel(max_scores[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from keras.layers.recurrent import Bidirectional, GRU, LSTM, SimpleRNN\n",
    "from keras.layers import containers\n",
    "from keras.layers.core import AutoEncoder, TimeDistributedDense\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "encoder = Sequential()\n",
    "encoder.add(Bidirectional(GRU, 8856, EMBEDDING_SIZE, activation='relu', inner_activation='relu', return_sequences=True))\n",
    "encoder.add(TimeDistributedDense(EMBEDDING_SIZE, 8856, activation=\"softmax\"))\n",
    "encoder.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = vocab.sentences[vocab.img_to_sent[3][3]]\n",
    "sentence_1h = np.array(vocab.sentence_to_1h(sentence))\n",
    "sentence_1h = sentence_1h[np.newaxis, :, :]\n",
    "#encoder.fit(sentence_1h, sentence_1h, nb_epoch=14, batch_size=16, show_accuracy=True)\n",
    "encoder.predict(sentence_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from keras.layers.recurrent import Bidirectional, GRU, LSTM, SimpleRNN\n",
    "from keras.layers import containers\n",
    "from keras.layers.core import AutoEncoder, TimeDistributedDense\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "encoder = Sequential()\n",
    "encoder.add(Bidirectional(GRU, vocab.token_count, EMBEDDING_SIZE, activation='relu', inner_activation='relu', return_sequences=True))\n",
    "encoder.add(TimeDistributedDense(EMBEDDING_SIZE, vocab.token_count, activation=\"softmax\"))\n",
    "encoder.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys = map(lambda y: map(vocab.get_1h, y), sent_matrix[:5000])\n",
    "ys = np.asarray(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder.fit(sent_matrix[:5000], ys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
