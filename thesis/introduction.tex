\section{Introduction}
People with a visual impairment are not able to read text or reliably interpret visual input. For this target group special types of books, such as braille books or audio books, are made to enable these people to still perform the same task but then via a different medium. 

Similarly, user interfaces of programs on computers are often enhanced in such a way that screen readers can easily read out what is being displayed on the screen. Examples of these enhancements are alternative texts for images or reorganizing the layout such that only relevant information is given to the screen reader. However, in a study about the frustrations of screen reader users on the web the authors \cite{lazar2007frustrates} aggregated a list of causes of frustration from 100 blind users. These users have a screen reader which is text-to-speech software. This list showed that the often used enhancements, such as alternative text for images, are not always enforced by websites. The top causes of frustrations include (among others) the layout that causes out of order auditory output from the screen reader, poorly designed forms and no alternative text for images. 

The study about the frustrations of screen reader users on the web shows that one cannot rely on external sources to provide a user friendly experience, such as a well formatted layout or an additional description of the images. Not only on the web, but also in magazines or newspapers there are often images that are not or limited augmented with textual descriptions and thus are useless for people with a visual impairment. However, multiple fields within artificial intelligence, such as computer vision, machine learning and linguistics, can be used to help the end-user with providing a computer generated description of an image. Literature shows many studies involving object recognition and classification \cite{carbonetto2004statistical, he2015delving} using convolutional neural networks (CNNs). In these studies objects ranging from inanimate to animate in images are classified, which results in a label that describes to which class each object belongs to. The most recent study \cite{he2015delving} claims a performance that even surpasses human performance on classifying images. 
While this study focusses on the image classification by labeling images, other studies \cite{mao2014explain, mitchell2012midge, Yang2011, Farhadi2010} have focused on the generation of sentences from images by combining a computer vision approach with linguistics. For description generation often two types of approaches are used in the literature. The first type is connects the grammar of a sentence in a description to an object or a relation between objects \cite{karpathy2014deep}. Models that use this approach generate sentences for the description that are following the syntactically correctness of the language grammar. The description generation model that is mentioned in \cite{mitchell2012midge} follows this first approach. It is trained on a the Flickr dataset which consists of images and descriptions. In order to generate meaningful sentences, the model uses co-occurence statistics to compute the probability distribution within a noun phrase. Furthermore, the characteristics of visually descriptive text are inspected to determine what generally the structure is of this type of text. These statistics are then used in the model along with the computer vision input (number of objects, labels) to generate novel sentences.
The second type of approach is using probabilistic machine learning to learn the probability density over multimodal input such as text and images. These models also generate sentences for the description, but are not according to a grammar. This results into more expressive sentences, but may contain less sound grammatical structures. The model in \cite{mao2014explain} is according to this second approach and the authors describe the model which consists of a multimodal Recurrent Neural Network that consists of six layers and which is trained on datasets from Flickr (Flickr-8K and Flickr-30K). What this network makes multimodal network is the multimodal layer. This layer connects the word representations layer with the image feature extraction network that is finally combined into a multimodal feature vector. 

% more in-depth with karpathy2014deep
