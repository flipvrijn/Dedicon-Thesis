\section{Introduction}
People with a visual impairment are not able to read text or reliably interpret visual input. For this target group special types of books, such as braille books or audio books, are made to enable these people to still perform the same task but then via a different medium. 

Similarly, user interfaces of programs on computers are often enhanced in such a way that screen readers, which are text-to-speech software, can easily read out what is being displayed on the screen. Examples of these enhancements are alternative texts for images or reorganizing the layout such that only relevant information is given to the screen reader. However, in a study about the frustrations of screen reader users on the web \citeauthor{lazar2007frustrates} \cite{lazar2007frustrates} aggregated a list of causes of frustration from 100 blind users. These users have a screen reader. This list showed that the often used enhancements, such as alternative text for images, are not always enforced by websites. The top causes of frustrations include (among others) the layout that causes out of order auditory output from the screen reader, poorly designed forms and no alternative text for images. 

The study about the frustrations of screen reader users on the web shows that one cannot rely on external sources to provide a user friendly experience, such as a well formatted layout or an additional description of the images. Not only on the web, but also in magazines or newspapers there are often images that are not or limited augmented with textual descriptions and thus are useless for people with a visual impairment. However, multiple fields within artificial intelligence, such as computer vision, machine learning and linguistics, can be used to help the end-user with providing a computer generated description of an image. 

\subsection{Related work}
Literature shows many studies involving object recognition and classification \cite{carbonetto2004statistical, he2015delving} using convolutional neural networks (CNNs). In these studies objects ranging from inanimate to animate in images are classified, which results in a label that describes to which class each object belongs to. The most recent study \cite{he2015delving} claims a performance that even surpasses human performance on classifying images. 
While this study focuses on the image classification by labeling images, other studies \cite{mao2014explain, mitchell2012midge, Yang2011, Farhadi2010} have focused on the generation of sentences from images by combining a computer vision approach with linguistic models. For description generation often two types of approaches are used in the literature. The first type is connecting the grammar of a sentence in a description to an object or a relation between objects \cite{karpathyjoulin2014deep}. Models that use this approach generate sentences for the description that are following the syntactically correctness of the language grammar. The description generation model that is mentioned in \cite{mitchell2012midge} follows this first approach. It is trained on the Flickr datasets which consists of images and descriptions. In order to generate meaningful sentences, the model uses co-occurrence statistics to compute the probability distribution within a noun phrase. Furthermore, the characteristics of visually descriptive text are inspected to determine what generally the structure is of this type of text. These statistics are then used in the model along with the computer vision input (number of objects, labels) to generate novel sentences.

The second type of approach is using probabilistic machine learning to learn the probability density over multimodal input such as text and images. These models also generate sentences for the description, but are not according to a grammar. This results into more expressive sentences, but may contain less sound grammatical structures. The models in \cite{mao2014explain,karpathyfeifei2014deep,karpathyjoulin2014deep} are according to this second approach and the authors describe the model which consists of a multimodal Recurrent Neural Network. What this network makes multimodal network is the multimodal layer. This layer connects the word representations layer with the image feature extraction network that is finally combined into a multimodal feature vector. 

\subsection{Existing model}
The overall architecture of the model that will be used is based on the model by \citeauthor{karpathyfeifei2014deep} where a CNN is combined with a Recurrent Neural Network \cite{karpathyfeifei2014deep} or by \citeauthor{vinyals2014show} where a CNN is combined with a Long-Short Term Memory (LSTM) network \cite{vinyals2014show}. Both approaches use a CNN by \citeauthor{simonyan2014very} \cite{simonyan2014very} to extract features from an image and a model which can integrate the image features and a sequence of words.  

When an LSTM model is compared to an RNN model an LSTM model tends to outperform the other model on sequence tasks such as generating sentences from images or translation \cite{vinyals2014show}. 

In the next sections the CNN is explained in more detail as well as the LSTM approach for combining the image and text.

\subsubsection{Image features}
The CNN part of the multimodal model is designed by \citeauthor{simonyan2014very}. They participated in the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) 2014 as the Visual Geometry Group (VGG) with a very deep convolution network and finished with the first and second place. Prior to their work, CNNs consisted of only a few layers and convolution filters with large receptive fields were used. However, \citeauthor{simonyan2014very} took another approach by lowering the size of the convolution filters to 3\texttimes 3 in all layers and by increasing the number of weight layers to 16 or 19. These architectures were not only able to achieve state-of-the-art accuracy on the classification and localization tasks, but were also able to generalize to other datasets.

The last layer of the model has 1000 output units that represent the 1000 categories on which the model has been trained. The activation of each of those units are probabilities for each category to be present in the image. 

In order to integrate this model in the overall multimodal model, the classification layers are stripped off to expose the rectified linear unit (ReLU) in order to get the more rich internal representation of 4096 abstract features.

\subsubsection{Combining image and text}
\citeauthor{vinyals2014show} use an LSTM model to generate sentences from images. The main component of such a model is the memory cell which contains information at every time step about the inputs that have been given up until this step. Furthermore, three binary gates govern the behavior of the model by telling the cell to forget the current value, read its input and outputting its new value. A general overview of such a model is shown in Figure~\ref{fig:lstm} where each part itself and the relation with other parts are depicted. The $\odot$ nodes indicate a multiplication with the gate values.

\begin{figure}[h!]
\centering
	\includegraphics[width=0.5\textwidth]{images/LSTM}
	\caption{Illustration of an LSTM model adopted from \cite{vinyals2014show}: Memory cell $C$ which is controlled by three gates $\sigma_{output}$, $\sigma_{input}$ and $\sigma_{forget}$. The blue lines indicate the recurrent connections from the output $m_{t-1}$ back to the gates at time $t$.}
	\label{fig:lstm}
\end{figure}

The image $I$ is fed into the model ($x$) only at the initial step. After this step the sequence of words in the ground truth sentence $S = (S_0,\ldots,S_N)$ is the input of the model. To mark the start and the ending of the sentence, $S_0$ and $S_N$ are special tokens. The words in the sentence are encoded as a 1-of-$K$ vector, where $K$ is the number of words in the entire vocabulary (training set) and the 1 is the index of the word in the vocabulary.

\begin{equation}\label{eq:loss-function-lstm}
	L(I,S)=-\sum\limits_{t=1}^{N}\log p_t(S_t)
\end{equation}

The output of the model is a probability distribution $p_t$ over all words in the vocabulary. Finally, the loss function formalized in Equation~\ref{eq:loss-function-lstm} is minimized taking into account all the parameters of the model, the image features from the CNN and the word encodings.