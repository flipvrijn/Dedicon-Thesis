@article{lazar2007frustrates,
author = {Lazar, Jonathan and Allen, Aaron and Kleinman, Jason and Malarkey, Chris},
journal = {International Journal of human-computer interaction},
number = {3},
pages = {247--269},
publisher = {Taylor \& Francis},
title = {{What frustrates screen reader users on the web: A study of 100 blind users}},
volume = {22},
year = {2007}
}
@misc{rohen1993virtual,
annote = {US Patent 5,186,629},
author = {Rohen, James E},
publisher = {Google Patents},
title = {{Virtual graphics display capable of presenting icons and windows to the blind computer user and method}},
year = {1993}
}
@inproceedings{mitchell2012midge,
abstract = {This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections. By leveraging syntactically informed word co-occurrence statistics, the generator Ô¨Ålters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.},
author = {Mitchell, Margaret and Dodge, Jesse and Goyal, Amit and Yamaguchi, Kota and Stratos, Karl and Mensch, Alyssa and Berg, Alex and Han, Xufeng and Berg, Tamara and Health, Oregon and Dodge, Jesse and Mensch, Alyssa and Goyal, Amit and Berg, Alex and Yamaguchi, Kota and Berg, Tamara and Stratos, Karl and {Daum\'{e} III}, Hal},
booktitle = {Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Mitchell et al. - 2012 - Midge Generating Image Descriptions From Computer Vision Detections.pdf:pdf},
isbn = {978-1-937284-19-0},
organization = {Association for Computational Linguistics},
pages = {747--756},
title = {{Midge: Generating image descriptions from computer vision detections}},
year = {2012}
}
@article{russakovsky2014imagenet,
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Others},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Russakovsky et al. - 2014 - Imagenet large scale visual recognition challenge.pdf:pdf},
journal = {arXiv preprint arXiv:1409.0575},
title = {{Imagenet large scale visual recognition challenge}},
year = {2014}
}
@article{mao2014explain,
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan L},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Mao et al. - 2014 - Explain images with multimodal recurrent neural networks.pdf:pdf},
journal = {arXiv preprint arXiv:1410.1090},
title = {{Explain images with multimodal recurrent neural networks}},
year = {2014}
}
@inproceedings{Farhadi2010,
abstract = {Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned using data. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.},
author = {Farhadi, Ali and Hejrati, Mohsen and Sadeghi, Mohammad Amin and Young, Peter and Rashtchian, Cyrus and Hockenmaier, Julia and Forsyth, David},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15561-1\_2},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Farhadi et al. - 2010 - Every picture tells a story Generating sentences from images.pdf:pdf},
isbn = {364215560X},
issn = {03029743},
number = {PART 4},
pages = {15--29},
title = {{Every picture tells a story: Generating sentences from images}},
volume = {6314 LNCS},
year = {2010}
}
@article{he2015delving,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we de-rive a robust initialization method that particularly consid-ers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classifica-tion dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [29]). To our knowledge, our result is the first to surpass human-level per-formance (5.1\%, [22]) on this visual recognition challenge.},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1502.01852},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - Unknown - Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf:pdf},
journal = {arXiv preprint arXiv:1502.01852},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
year = {2015}
}
@article{karpathyfeifei2014deep,
author = {Karpathy, Andrej and Fei-Fei, Li},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Karpathy, Fei-Fei - 2014 - Deep visual-semantic alignments for generating image descriptions.pdf:pdf},
journal = {arXiv preprint arXiv:1412.2306},
title = {{Deep visual-semantic alignments for generating image descriptions}},
year = {2014}
}
@inproceedings{karpathyjoulin2014deep,
abstract = {We introduce a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. In addition to a ranking objective seen in previous work, this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions since the inferred inter-modal fragment alignment is explicit.},
archivePrefix = {arXiv},
arxivId = {1406.5679},
author = {Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1406.5679},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Karpathy, Joulin, Li - 2014 - Deep fragment embeddings for bidirectional image sentence mapping.pdf:pdf},
pages = {1--9},
title = {{Deep Fragment Embeddings for Bidirectional Image Sentence Mapping}},
url = {http://arxiv.org/abs/1406.5679},
year = {2014}
}
@inproceedings{Yang2011,
abstract = {We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gi- gaword corpus to obtain their estimates; to- gether with probabilities of co-located nouns, scenes and prepositions. We use these esti- mates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image de- tections as the emissions. Experimental re- sults show that our strategy of combining vi- sion and language produces readable and de- scriptive sentences compared to naive strate- gies that use vision alone.},
author = {Yang, Yezhou and Teo, Ching Lik and Daume, Hal and Aloimonos, Yiannis},
booktitle = {Proceedings of EMNLP},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2011 - Corpus-Guided Sentence Generation of Natural Images.pdf:pdf},
isbn = {1937284115},
pages = {444--454},
title = {{Corpus-Guided Sentence Generation of Natural Images}},
year = {2011}
}
@article{Papineni2002,
abstract = {Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wj},
doi = {10.3115/1073083.1073135},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Papineni et al. - 2002 - BLEU a method for automatic evaluation of machine translation.pdf:pdf},
issn = {00134686},
journal = {\ldots of the 40Th Annual Meeting on \ldots},
number = {July},
pages = {311--318},
title = {{BLEU: a method for automatic evaluation of machine translation}},
url = {http://dl.acm.org/citation.cfm?id=1073135},
year = {2002}
}
@inproceedings{lin2004automatic,
author = {Lin, Chin-Yew and Och, Franz Josef},
booktitle = {Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics},
organization = {Association for Computational Linguistics},
pages = {605},
title = {{Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics}},
year = {2004}
}
@article{Lin2004,
author = {Lin, Chin-Yew and Och, Franz Josef},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Och - 2004 - Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics.pdf:pdf},
journal = {Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL)},
title = {{Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics}},
year = {2004}
}
@article{vinyals2014show,
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
file = {:Users/Flip/Downloads/1411.4555v2.pdf:pdf},
journal = {arXiv preprint arXiv:1411.4555},
title = {{Show and tell: A neural image caption generator}},
year = {2014}
}
@article{simonyan2014very,
author = {Simonyan, Karen and Zisserman, Andrew},
file = {:Users/Flip/Downloads/1409.1556.pdf:pdf},
journal = {arXiv preprint arXiv:1409.1556},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2014}
}
@article{Kiros2013,
abstract = {Abstract We introduce two multimodal neural language models : models of natural language that can be conditioned on other modalities. An image-text multimodal neural language model can be used to retrieve images given complex sentence queries, retrieve phrase ... $\backslash$n},
author = {Kiros, R and Zemel, R and Salakhutdinov, R},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Kiros, Zemel, Salakhutdinov - 2013 - Multimodal Neural Language Models.pdf:pdf},
journal = {Proc NIPS Deep Learning \ldots},
keywords = {Image Tag Inference},
title = {{Multimodal Neural Language Models}},
url = {http://www.cs.toronto.edu/~rkiros/papers/mnlm2014.pdf$\backslash$npapers3://publication/uuid/00AE85FB-98DC-4EB9-A6E8-A423C47C0B98},
year = {2013}
}
@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll\'{a}r, Piotr and Zitnick, C Lawrence},
doi = {10.1007/978-3-319-10602-1\_48},
eprint = {1405.0312},
isbn = {978-3-319-10601-4},
journal = {arXiv preprint arXiv \ldots},
title = {{Microsoft COCO: Common Objects in Context}},
url = {http://arxiv.org/abs/1405.0312v2$\backslash$npapers3://publication/uuid/F5BB1978-0B9E-4127-9863-85A71B8F8225},
volume = {cs.CV},
year = {2014}
}
@inproceedings{Russakovsky2012,
abstract = {We consider the task of learning visual connections between object categories using the ImageNet dataset, which is a large-scale dataset ontology containing more than 15 thousand object classes. We want to discover visual relationships between the classes that are currently missing (such as similar colors or shapes or textures). In this work we learn 20 visual attributes and use themin a zero-shot transfer learning experiment as well as to make visual connections between semantically unrelated object categories.},
author = {Russakovsky, Olga and Fei-Fei, Li},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-35749-7\_1},
isbn = {978-3-642-35748-0},
issn = {03029743},
number = {PART 1},
pages = {1--14},
title = {{Attribute learning in large-scale datasets}},
volume = {6553 LNCS},
year = {2012}
}
@incollection{carbonetto2004statistical,
author = {Carbonetto, Peter and de Freitas, Nando and Barnard, Kobus and Freitas, N and Barnard, Kobus},
booktitle = {Computer Vision-ECCV 2004},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Carbonetto, Freitas, Barnard - 2004 - A statistical model for general contextual object recognition.pdf:pdf},
pages = {350--362},
publisher = {Springer},
title = {{A statistical model for general contextual object recognition}},
url = {papers3://publication/uuid/48FBF7F2-62EE-4E3C-95C6-0F73608EE2A6},
year = {2004}
}
