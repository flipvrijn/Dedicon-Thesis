@incollection{carbonetto2004statistical,
author = {Carbonetto, Peter and de Freitas, Nando and Barnard, Kobus and Freitas, N and Barnard, Kobus},
booktitle = {Computer Vision-ECCV 2004},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Carbonetto, Freitas, Barnard - 2004 - A statistical model for general contextual object recognition.pdf:pdf},
pages = {350--362},
publisher = {Springer},
title = {{A statistical model for general contextual object recognition}},
year = {2004}
}
@article{he2015delving,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra com-putational cost and little overfitting risk. Second, we de-rive a robust initialization method that particularly consid-ers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classifica-tion dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [29]). To our knowledge, our result is the first to surpass human-level per-formance (5.1\%, [22]) on this visual recognition challenge.},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1502.01852},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - Unknown - Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf:pdf},
journal = {arXiv preprint arXiv:1502.01852},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
year = {2015}
}
@inproceedings{mitchell2012midge,
abstract = {This paper introduces a novel generation system that composes humanlike descriptions of images from computer vision detections. By leveraging syntactically informed word co-occurrence statistics, the generator Ô¨Ålters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date.},
author = {Mitchell, Margaret and Dodge, Jesse and Goyal, Amit and Yamaguchi, Kota and Stratos, Karl and Mensch, Alyssa and Berg, Alex and Han, Xufeng and Berg, Tamara and Health, Oregon and Dodge, Jesse and Mensch, Alyssa and Goyal, Amit and Berg, Alex and Yamaguchi, Kota and Berg, Tamara and Stratos, Karl and {Daum\'{e} III}, Hal},
booktitle = {Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Mitchell et al. - 2012 - Midge Generating Image Descriptions From Computer Vision Detections.pdf:pdf},
isbn = {978-1-937284-19-0},
organization = {Association for Computational Linguistics},
pages = {747--756},
title = {{Midge: Generating image descriptions from computer vision detections}},
year = {2012}
}
@misc{rohen1993virtual,
annote = {US Patent 5,186,629},
author = {Rohen, James E},
publisher = {Google Patents},
title = {{Virtual graphics display capable of presenting icons and windows to the blind computer user and method}},
year = {1993}
}
@article{lazar2007frustrates,
author = {Lazar, Jonathan and Allen, Aaron and Kleinman, Jason and Malarkey, Chris},
journal = {International Journal of human-computer interaction},
number = {3},
pages = {247--269},
publisher = {Taylor \& Francis},
title = {{What frustrates screen reader users on the web: A study of 100 blind users}},
volume = {22},
year = {2007}
}
@inproceedings{Farhadi2010,
abstract = {Humans can prepare concise descriptions of pictures, focusing on what they find important. We demonstrate that automatic methods can do so too. We describe a system that can compute a score linking an image to a sentence. This score can be used to attach a descriptive sentence to a given image, or to obtain images that illustrate a given sentence. The score is obtained by comparing an estimate of meaning obtained from the image to one obtained from the sentence. Each estimate of meaning comes from a discriminative procedure that is learned using data. We evaluate on a novel dataset consisting of human-annotated images. While our underlying estimate of meaning is impoverished, it is sufficient to produce very good quantitative results, evaluated with a novel score that can account for synecdoche.},
author = {Farhadi, Ali and Hejrati, Mohsen and Sadeghi, Mohammad Amin and Young, Peter and Rashtchian, Cyrus and Hockenmaier, Julia and Forsyth, David},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15561-1\_2},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Farhadi et al. - 2010 - Every picture tells a story Generating sentences from images.pdf:pdf},
isbn = {364215560X},
issn = {03029743},
number = {PART 4},
pages = {15--29},
title = {{Every picture tells a story: Generating sentences from images}},
volume = {6314 LNCS},
year = {2010}
}
@inproceedings{Yang2011,
abstract = {We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gi- gaword corpus to obtain their estimates; to- gether with probabilities of co-located nouns, scenes and prepositions. We use these esti- mates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image de- tections as the emissions. Experimental re- sults show that our strategy of combining vi- sion and language produces readable and de- scriptive sentences compared to naive strate- gies that use vision alone.},
author = {Yang, Yezhou and Teo, Ching Lik and Daume, Hal and Aloimonos, Yiannis},
booktitle = {Proceedings of EMNLP},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2011 - Corpus-Guided Sentence Generation of Natural Images.pdf:pdf},
isbn = {1937284115},
pages = {444--454},
title = {{Corpus-Guided Sentence Generation of Natural Images}},
year = {2011}
}
@article{mao2014explain,
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Yuille, Alan L},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Mao et al. - 2014 - Explain images with multimodal recurrent neural networks.pdf:pdf},
journal = {arXiv preprint arXiv:1410.1090},
title = {{Explain images with multimodal recurrent neural networks}},
year = {2014}
}
@inproceedings{karpathyjoulin2014deep,
abstract = {We introduce a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. In addition to a ranking objective seen in previous work, this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions since the inferred inter-modal fragment alignment is explicit.},
archivePrefix = {arXiv},
arxivId = {1406.5679},
author = {Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1406.5679},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Karpathy, Joulin, Li - 2014 - Deep fragment embeddings for bidirectional image sentence mapping.pdf:pdf},
pages = {1--9},
title = {{Deep Fragment Embeddings for Bidirectional Image Sentence Mapping}},
year = {2014}
}
@article{russakovsky2014imagenet,
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Others},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Russakovsky et al. - 2014 - Imagenet large scale visual recognition challenge.pdf:pdf},
journal = {arXiv preprint arXiv:1409.0575},
title = {{Imagenet large scale visual recognition challenge}},
year = {2014}
}
@article{Lin2004,
author = {Lin, Chin-Yew and Och, Franz Josef},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Och - 2004 - Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics.pdf:pdf},
journal = {Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL)},
title = {{Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics}},
year = {2004}
}
@article{Papineni2002,
abstract = {Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wj},
doi = {10.3115/1073083.1073135},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Papineni et al. - 2002 - BLEU a method for automatic evaluation of machine translation.pdf:pdf},
issn = {00134686},
journal = {\ldots of the 40Th Annual Meeting on \ldots},
number = {July},
pages = {311--318},
title = {{BLEU: a method for automatic evaluation of machine translation}},
year = {2002}
}
@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll\'{a}r, Piotr and Zitnick, C Lawrence},
doi = {10.1007/978-3-319-10602-1\_48},
eprint = {1405.0312},
isbn = {978-3-319-10601-4},
journal = {arXiv preprint arXiv \ldots},
title = {{Microsoft COCO: Common Objects in Context}},
volume = {cs.CV},
year = {2014}
}
@inproceedings{Russakovsky2012,
abstract = {We consider the task of learning visual connections between object categories using the ImageNet dataset, which is a large-scale dataset ontology containing more than 15 thousand object classes. We want to discover visual relationships between the classes that are currently missing (such as similar colors or shapes or textures). In this work we learn 20 visual attributes and use themin a zero-shot transfer learning experiment as well as to make visual connections between semantically unrelated object categories.},
author = {Russakovsky, Olga and Fei-Fei, Li},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-35749-7\_1},
isbn = {978-3-642-35748-0},
issn = {03029743},
number = {PART 1},
pages = {1--14},
title = {{Attribute learning in large-scale datasets}},
volume = {6553 LNCS},
year = {2012}
}
@article{Kiros2013,
abstract = {Abstract We introduce two multimodal neural language models : models of natural language that can be conditioned on other modalities. An image-text multimodal neural language model can be used to retrieve images given complex sentence queries, retrieve phrase ... $\backslash$n},
author = {Kiros, R and Zemel, R and Salakhutdinov, R},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Kiros, Zemel, Salakhutdinov - 2013 - Multimodal Neural Language Models.pdf:pdf},
journal = {Proc NIPS Deep Learning \ldots},
keywords = {Image Tag Inference},
title = {{Multimodal Neural Language Models}},
year = {2013}
}
@article{karpathyfeifei2014deep,
author = {Karpathy, Andrej and Fei-Fei, Li},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Karpathy, Fei-Fei - 2014 - Deep visual-semantic alignments for generating image descriptions.pdf:pdf},
journal = {arXiv preprint arXiv:1412.2306},
title = {{Deep visual-semantic alignments for generating image descriptions}},
year = {2014}
}
@article{vinyals2014show,
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
file = {:Users/Flip/Downloads/1411.4555v2.pdf:pdf},
journal = {arXiv preprint arXiv:1411.4555},
title = {{Show and tell: A neural image caption generator}},
year = {2014}
}
@article{simonyan2014very,
author = {Simonyan, Karen and Zisserman, Andrew},
file = {:Users/Flip/Downloads/1409.1556.pdf:pdf},
journal = {arXiv preprint arXiv:1409.1556},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2014}
}
@article{zaremba2014recurrent,
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
file = {:Users/Flip/Downloads/1409.2329.pdf:pdf},
journal = {arXiv preprint arXiv:1409.2329},
title = {{Recurrent neural network regularization}},
year = {2014}
}
@inproceedings{villegas2014overview,
author = {Villegas, Mauricio and Paredes, Roberto},
booktitle = {CLEF 2014 Evaluation Labs and Workshop, Online Working Notes},
file = {:Users/Flip/Downloads/CLEF2014wn-Image-VillegasEt2014.pdf:pdf},
title = {{Overview of the ImageCLEF 2014 scalable concept image annotation task}},
year = {2014}
}
@article{mao2015learning,
author = {Mao, Junhua and Xu, Wei and Yang, Yi and Wang, Jiang and Huang, Zhiheng and Yuille, Alan},
file = {:Users/Flip/Downloads/1504.06692.pdf:pdf},
journal = {arXiv preprint arXiv:1504.06692},
title = {{Learning like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images}},
year = {2015}
}
@article{vedantam2014cider,
author = {Vedantam, Ramakrishna and Zitnick, C Lawrence and Parikh, Devi},
file = {:Users/Flip/Downloads/1411.5726.pdf:pdf},
journal = {arXiv preprint arXiv:1411.5726},
title = {{CIDEr: Consensus-based Image Description Evaluation}},
year = {2014}
}
@inproceedings{la1998combining,
author = {{La Cascia}, Marco and Sethi, Saratendu and Sclaroff, Stan},
booktitle = {Content-Based Access of Image and Video Libraries, 1998. Proceedings. IEEE Workshop on},
file = {:Users/Flip/Downloads/1998-004-combining-text-and-vis-cues.pdf:pdf},
organization = {IEEE},
pages = {24--28},
title = {{Combining textual and visual cues for content-based image retrieval on the world wide web}},
year = {1998}
}
@article{Schuster1997,
abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported View full abstract},
author = {Schuster, Mike and Paliwal, Kuldip K},
doi = {10.1109/78.650093},
file = {:Users/Flip/Downloads/Schuster97\_BRNN.pdf:pdf},
issn = {1053-587X},
journal = {Signal Processing, IEEE Transactions on},
number = {11},
pages = {2673--2681},
title = {{Bidirectional recurrent neural networks}},
volume = {45},
year = {1997}
}
@article{Sande2011,
abstract = {For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised and (2) appearance and immediate nearby context are most effective for object recognition. Our method is class-independent and is shown to cover 96.7\% of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image. Our selective search enables the use of the more expensive bag-of-words method which we use to substantially improve the state-of-the-art by up to 8.5\% for 8 out of 20 classes on the Pascal VOC 2010 detection challenge. $\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t $\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t $\backslash$t $\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t $\backslash$t $\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t $\backslash$t $\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t $\backslash$t$\backslash$t$\backslash$t $\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t $\backslash$t$\backslash$t$\backslash$t$\backslash$t $\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t $\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t $\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$t $\backslash$t$\backslash$t$\backslash$t$\backslash$t$\backslash$tView full abstract\&\#187;},
author = {Sande, Kea Van De},
doi = {10.1109/ICCV.2011.6126456},
file = {:Users/Flip/Downloads/vandeSandeICCV2011.pdf:pdf},
isbn = {978-1-4577-1100-8},
issn = {1550-5499},
journal = {IEEE International Conference on Computer Vision},
number = {2},
pages = {1879--1886},
title = {{Segmentation as selective search for object recognition}},
year = {2011}
}
@inproceedings{zhu2010latent,
author = {Zhu, Long and Chen, Yuanhao and Yuille, Alan and Freeman, William},
booktitle = {Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on},
organization = {IEEE},
pages = {1062--1069},
title = {{Latent hierarchical structural learning for object detection}},
year = {2010}
}
@article{felzenszwalb2010object,
author = {Felzenszwalb, Pedro F and Girshick, Ross B and McAllester, David and Ramanan, Deva},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
number = {9},
pages = {1627--1645},
publisher = {IEEE},
title = {{Object detection with discriminatively trained part-based models}},
volume = {32},
year = {2010}
}
@article{girshick2015fast,
author = {Girshick, Ross},
file = {:Users/Flip/Downloads/1504.08083v1.pdf:pdf},
journal = {arXiv preprint arXiv:1504.08083},
title = {{Fast R-CNN}},
year = {2015}
}
@inproceedings{Gilbert15_CLEF,
address = {Toulouse, France},
author = {Gilbert, Andrew and Piras, Luca and Wang, Josiah and Yan, Fei and Dellandrea, Emmanuel and Gaizauskas, Robert and Villegas, Mauricio and Mikolajczyk, Krystian},
booktitle = {CLEF2015 Working Notes},
issn = {1613-0073},
month = sep,
publisher = {CEUR-WS.org},
series = {\{CEUR\} Workshop Proceedings},
title = {{Overview of the ImageCLEF 2015 Scalable Image Annotation, Localization and Sentence Generation task}},
year = {2015}
}
@inproceedings{Rabinovich2007,
abstract = {In the task of visual object categorization, semantic context can play the very important role of reducing ambiguity in objects' visual appearance. In this work we propose to incorporate semantic object context as a post-processing step into any off-the-shelf object categorization model. Using a conditional random field (CRF) framework, our approach maximizes object label agreement according to contextual relevance. We compare two sources of context: one learned from training data and another queried from Google Sets. The overall performance of the proposed framework is evaluated on the PASCAL and MSRC datasets. Our findings conclude that incorporating context into object categorization greatly improves categorization accuracy.},
author = {Rabinovich, Andrew and Vedaldi, Andrea and Galleguillos, Carolina and Wiewiora, Eric and Belongie, Serge},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2007.4408986},
file = {:Users/Flip/Downloads/10.1.1.310.7301.pdf:pdf},
isbn = {978-1-4244-1631-8},
issn = {1550-5499},
title = {{Objects in context}},
year = {2007}
}
@article{Modestino1992,
abstract = {An image is segmented into a collection of disjoint regions that
form the nodes of an adjacency graph, and image interpretation is
achieved through assigning object labels (or interpretations) to the
segmented regions (or nodes) using domain knowledge, extracted feature
measurements, and spatial relationships between the various regions. The
interpretation labels are modeled as a Markov random field (MRF) on the
corresponding adjacency graph, and the image interpretation problem is
then formulated as a maximum a posteriori (MAP) estimation rule, given
domain knowledge and region-based measurements. Simulated annealing is
used to find this best realization or optimal MAP interpretation. This
approach also provides a systematic method for organizing and
representing domain knowledge through appropriate design of the clique
functions describing the Gibbs distribution representing the pdf of the
underlying MRF. A general methodology is provided for the design of the
clique functions. Results of image interpretation experiments on
synthetic and real-world images are described},
author = {Modestino, J.W. and Zhang, J.},
doi = {10.1109/34.141552},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {6},
title = {{A Markov random field model-based approach to image interpretation}},
volume = {14},
year = {1992}
}
@article{lavie2014meteor,
author = {Lavie, Michael Denkowski Alon},
file = {:Users/Flip/Downloads/meteor-1.5.pdf:pdf},
journal = {ACL 2014},
pages = {376},
title = {{Meteor Universal: Language Specific Translation Evaluation for Any Target Language}},
year = {2014}
}
@article{Lin2004a,
abstract = {ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to auto- matically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of over- lapping units such as n-gram, word sequences, and word pairs between the computer-generated sum- mary to be evaluated and the ideal summaries cre- ated by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summariza- tion evaluation package and their evaluatio ns. Three of them have been used in the Document Under- standing$\backslash$tConference$\backslash$t(DUC)$\backslash$t2004,$\backslash$ta$\backslash$tlarge -scale summarization evaluation sponsored by NIST.},
author = {Lin, C Y},
file = {:Users/Flip/Library/Application Support/Mendeley Desktop/Downloaded/Lin - 2004 - Rouge A package for automatic evaluation of summaries.pdf:pdf},
issn = {00036951},
journal = {Proceedings of the workshop on text summarization branches out (WAS 2004)},
pages = {25--26},
title = {{Rouge: A package for automatic evaluation of summaries}},
url = {papers2://publication/uuid/5DDA0BB8-E59F-44C1-88E6-2AD316DAEF85},
year = {2004}
}
@article{Fortu2005,
abstract = {Contextual information plays a key role in the automatic interpretation of text. This paper is concerned with the identification of textual contexts. A context taxonomy is introduced first, followed by an algorithm for detecting context boundaries, Experiments on the detection of subjective contexts using a machine learning model were performed using a set of syntactic features.},
author = {Fortu, Ovidiu and Moldovan, Dan},
doi = {10.1007/11508373\_13},
file = {:Users/Flip/Downloads/j05dmof.pdf:pdf},
isbn = {978-3-540-26924-3},
issn = {03029743},
journal = {Modeling and Using Context},
pages = {169--182},
title = {{Identification of textual contexts}},
url = {http://link.springer.com/chapter/10.1007/11508373\_13},
year = {2005}
}
